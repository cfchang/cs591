{
 "metadata": {
  "name": "",
  "signature": "sha256:5f906ff3f89ab3c5ee554649a380d87be4aa94a11c9ed698ae2f658eb82d7bbf"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "__author__ = 'acpigeon'\n",
      "import json\n",
      "import random\n",
      "import datetime\n",
      "import math\n",
      "import csv\n",
      "import joblib\n",
      "import nltk\n",
      "\n",
      "import numpy as np\n",
      "from scipy.stats import randint as sp_randint\n",
      "from scipy.stats import uniform\n",
      "from nltk.stem.snowball import PorterStemmer\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.preprocessing import scale\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "\n",
      "def load_data(filename, max_neg_class=float(\"inf\")):\n",
      "    \"\"\"\n",
      "    request_id: text, key\n",
      "    requester_number_of_comments_at_request: int\n",
      "    requester_number_of_comments_in_raop_at_request: int\n",
      "    requester_number_of_posts_at_request: int\n",
      "    requester_number_of_posts_on_raop_at_request: int\n",
      "    requester_number_of_subreddits_at_request: int\n",
      "    requester_upvotes_minus_downvotes_at_request\": int\n",
      "    requester_upvotes_plus_downvotes_at_request: int\n",
      "    requester_account_age_in_days_at_request: float\n",
      "    requester_days_since_first_post_on_raop_at_request: float\n",
      "    unix_timestamp_of_request: float\n",
      "    requester_subreddits_at_request: list of strings\n",
      "    request_text_edit_aware: string\n",
      "    request_title: string\n",
      "    \"\"\"\n",
      "    input_file = open(filename, 'r')\n",
      "    file_contents = input_file.read()\n",
      "    raw_data = json.loads(file_contents)\n",
      "    random.shuffle(raw_data)  # Shuffle the data now before we transform it\n",
      "\n",
      "    if 'requester_received_pizza' in raw_data[0].keys():  # this is the train set, downsample neg class\n",
      "        neg_class_count = 0\n",
      "        downsampled_data = []\n",
      "        for example in raw_data:\n",
      "            if example['requester_received_pizza'] is True or neg_class_count < max_neg_class:\n",
      "                downsampled_data.append(example)\n",
      "                if example['requester_received_pizza'] is False:\n",
      "                    neg_class_count += 1\n",
      "        return downsampled_data\n",
      "    else:\n",
      "        return raw_data\n",
      "\n",
      "\n",
      "def build_num_features_matrix(data_set):\n",
      "    \"\"\"\n",
      "    Returns an n x 9 matrix of all numeric features.\n",
      "    \"\"\"\n",
      "    n = len(data_set)\n",
      "    mat = np.zeros((n, 9))\n",
      "    for i in xrange(n):\n",
      "        mat[i][0] = data_set[i]['requester_number_of_comments_at_request']\n",
      "        mat[i][1] = data_set[i]['requester_number_of_comments_in_raop_at_request']\n",
      "        mat[i][2] = data_set[i]['requester_number_of_posts_at_request']\n",
      "        mat[i][3] = data_set[i]['requester_number_of_posts_on_raop_at_request']\n",
      "        mat[i][4] = data_set[i]['requester_number_of_subreddits_at_request']\n",
      "        mat[i][5] = data_set[i]['requester_upvotes_minus_downvotes_at_request']\n",
      "        mat[i][6] = data_set[i]['requester_upvotes_plus_downvotes_at_request']\n",
      "        mat[i][7] = data_set[i]['requester_account_age_in_days_at_request']\n",
      "        mat[i][8] = data_set[i]['requester_days_since_first_post_on_raop_at_request']\n",
      "    return scale(mat)\n",
      "\n",
      "\n",
      "def build_date_features(data_set):\n",
      "    \"\"\"\n",
      "    For the date of posting (from unix_timestamp_of_request), convert to day of week and hours after midnight feature.\n",
      "    \"\"\"\n",
      "    n = len(data_set)\n",
      "    mat = np.zeros((n, 8))\n",
      "    date_to_columns = {'Mon': 0,  'Tue': 1, 'Wed': 2, 'Thu': 3, 'Fri': 4, 'Sat': 5, 'Sun': 6}\n",
      "    for i in xrange(n):\n",
      "        epoch_seconds = data_set[i]['unix_timestamp_of_request']\n",
      "        day = datetime.datetime.fromtimestamp(epoch_seconds).strftime('%a')\n",
      "        hours_after_midnight = datetime.datetime.fromtimestamp(epoch_seconds).strftime('%H')\n",
      "        mat[i][date_to_columns[day]] = 1\n",
      "        mat[i][7] = int(hours_after_midnight)\n",
      "        return mat\n",
      "\n",
      "\n",
      "def build_text_list_features(data_set):\n",
      "    \"\"\"\n",
      "    Convert list of text into categorical features, only used for subreddits here.\n",
      "    \"\"\"\n",
      "    n = len(data_set)\n",
      "    vectorizer = CountVectorizer()\n",
      "    lists_of_subreddits = []\n",
      "    for i in xrange(n):\n",
      "        lists_of_subreddits.append(' '.join(data_set[i]['requester_subreddits_at_request']))\n",
      "    mat = vectorizer.fit_transform(lists_of_subreddits)\n",
      "    return mat.todense()\n",
      "\n",
      "\n",
      "def get_meta(data_set, field_name):\n",
      "    \"\"\"\n",
      "    Returns an n x 1 array of the doc ids or labels.\n",
      "    Pass field_name = 'request_id' or 'requester_received_pizza'.\n",
      "    \"\"\"\n",
      "    n = len(data_set)\n",
      "    if field_name == 'request_id':\n",
      "        t = object\n",
      "    else:\n",
      "        t = float\n",
      "\n",
      "    mat = np.zeros((n, 1), dtype=t)\n",
      "    for idx in xrange(n):\n",
      "        mat[idx] = data_set[idx][field_name]\n",
      "    return mat\n",
      "\n",
      "\n",
      "def stem_tokens(tokens, stemmer):\n",
      "    stemmed = []\n",
      "    for item in tokens:\n",
      "        stemmed.append(stemmer.stem(item))\n",
      "    return stemmed\n",
      "\n",
      "\n",
      "def tokenize(text):\n",
      "    tokens = nltk.word_tokenize(text)\n",
      "    stems = stem_tokens(tokens, PorterStemmer())\n",
      "    return stems\n",
      "\n",
      "\n",
      "def generate_tfidf_matrix(train, test, field_name, _min_df=0.01, _max_df=0.7):\n",
      "    \"\"\"\n",
      "    Takes list of lists of text and returns the tfidf matrix.\n",
      "    Used for request_text_edit_aware, ....\n",
      "    \"\"\"\n",
      "    train_text, test_text = [], []\n",
      "    for t in train:\n",
      "        train_text.append(t[field_name])\n",
      "    for t in test:\n",
      "        test_text.append(t[field_name])\n",
      "\n",
      "    v = TfidfVectorizer(stop_words='english', min_df=_min_df, max_df=_max_df, tokenizer=tokenize)\n",
      "    v.fit(train_text + test_text)\n",
      "    return v.transform(train_text).todense(), v.transform(test_text).todense()\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # Load train data\n",
      "    train_data = load_data('train.json')\n",
      "    train_ids = get_meta(train_data, 'request_id')\n",
      "    train_numeric_features = build_num_features_matrix(train_data)\n",
      "    train_date_features = build_date_features(train_data)\n",
      "    train_subreddit_features = build_text_list_features(train_data)\n",
      "    train_labels = get_meta(train_data, 'requester_received_pizza')\n",
      "\n",
      "    # Load test data\n",
      "    test_data = load_data('test.json')\n",
      "    test_ids = get_meta(test_data, 'request_id')\n",
      "    test_numeric_features = build_num_features_matrix(test_data)\n",
      "    test_date_features = build_date_features(test_data)\n",
      "    test_subreddit_features = build_text_list_features(test_data)\n",
      "\n",
      "    # Train all tf features before messing with the data\n",
      "    tf_train_request, tf_test_request = generate_tfidf_matrix(train_data, test_data, 'request_text_edit_aware')\n",
      "    tf_train_title, tf_test_title = generate_tfidf_matrix(train_data, test_data, 'request_title')\n",
      "\n",
      "    # Combine all the features\n",
      "    train_feature_matrix = np.concatenate((train_numeric_features, train_date_features,\n",
      "                                           tf_train_request, tf_train_title), axis=1)\n",
      "    test_feature_matrix = np.concatenate((test_numeric_features, test_date_features,\n",
      "                                          tf_test_request, tf_test_title), axis=1)\n",
      "\n",
      "    X_train_all, X_test, y_train, y_test = train_test_split(train_feature_matrix, train_labels.ravel())\n",
      "\n",
      "    # In the test split, there is a pos/neg imbalance of ~ 730 to 2200\n",
      "    # Split the negative class into three roughly equal groups so we can train three different models and take the avg\n",
      "    # Methodology comes from EasyEnsemble approach from http://cse.seu.edu.cn/people/xyliu/publication/tsmcb09.pdf\n",
      "\n",
      "    X_train_neg_1, y_train_neg_1 = [], []\n",
      "    X_train_neg_2, y_train_neg_2 = [], []\n",
      "    X_train_neg_3, y_train_neg_3 = [], []\n",
      "    X_train_pos, y_train_pos = [], []\n",
      "\n",
      "    for s in zip(X_train_all, y_train):\n",
      "        if s[1] == 1.0:\n",
      "            X_train_pos.append(s[0])\n",
      "            y_train_pos.append(s[1])\n",
      "        else:\n",
      "            sorting_hat = random.choice([1, 2, 3])\n",
      "            if sorting_hat == 1:\n",
      "                X_train_neg_1.append(s[0])\n",
      "                y_train_neg_1.append(s[1])\n",
      "            elif sorting_hat == 2:\n",
      "                X_train_neg_2.append(s[0])\n",
      "                y_train_neg_2.append(s[1])\n",
      "            else:\n",
      "                X_train_neg_3.append(s[0])\n",
      "                y_train_neg_3.append(s[1])\n",
      "\n",
      "    # Then recombine each of the negative class subsets with the positive class\n",
      "    # This gives us three separate training groups of approximately equal split!\n",
      "\n",
      "    X_train_1 = np.array(X_train_neg_1 + X_train_pos)\n",
      "    y_train_1 = np.array(y_train_neg_1 + y_train_pos)\n",
      "\n",
      "    X_train_2 = np.array(X_train_neg_2 + X_train_pos)\n",
      "    y_train_2 = np.array(y_train_neg_2 + y_train_pos)\n",
      "\n",
      "    X_train_3 = np.array(X_train_neg_3 + X_train_pos)\n",
      "    y_train_3 = np.array(y_train_neg_3 + y_train_pos)\n",
      "\n",
      "    # Train the models\n",
      "\n",
      "    #1\n",
      "    lr = LogisticRegression(tol=0.01)\n",
      "    params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2']}\n",
      "    clf1 = GridSearchCV(lr, param_grid=params, scoring='roc_auc', verbose=True, cv=5, n_jobs=-1)\n",
      "    clf1.fit(X_train_1, y_train_1)\n",
      "    clf_1_x_val_predictions = clf1.predict(X_test)\n",
      "    class_rep_1 = classification_report(y_test, clf_1_x_val_predictions)\n",
      "    print clf1.best_params_\n",
      "    print class_rep_1\n",
      "\n",
      "\n",
      "    #2\n",
      "    svc = SVC()\n",
      "    svc_param_dist = {\"C\": uniform(),\n",
      "                         \"gamma\": uniform(),\n",
      "                         \"kernel\": ['linear', 'rbf'],\n",
      "                         \"class_weight\": [{1: 1}, {1: 2}, {1: 5}, {1: 10}],\n",
      "                         \"probability\": [True]\n",
      "                         }\n",
      "    #params = [{'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7],\n",
      "    #           'kernel': ['linear'], 'class_weight': [{1: 1}, {1: 5}, {1: 2}, {1: 3}, {1: 10}]}]\n",
      "\n",
      "    #clf2 = GridSearchCV(svc, param_grid=params, scoring='roc_auc', verbose=True, cv=5, n_jobs=-1)\n",
      "    clf2 = RandomizedSearchCV(svc, param_distributions=svc_param_dist, n_iter=100)\n",
      "\n",
      "    clf2.fit(X_train_2, y_train_2)\n",
      "    clf_2_x_val_predictions = clf2.predict(X_test)\n",
      "    class_rep_2 = classification_report(y_test, clf_2_x_val_predictions)\n",
      "    print clf2.best_params_\n",
      "    print class_rep_2\n",
      "\n",
      "    #3\n",
      "    gbc = GradientBoostingClassifier()\n",
      "    forest_param_dist = {\"max_depth\": [3,4,5,6,7],\n",
      "                               \"max_features\": sp_randint(1, 11),\n",
      "                               \"min_samples_split\": sp_randint(1, 11),\n",
      "                               \"min_samples_leaf\": sp_randint(1, 11),\n",
      "                               \"subsample\": uniform(),\n",
      "                               \"learning_rate\": uniform(),\n",
      "                               \"n_estimators\": sp_randint(1, 351)}\n",
      "\n",
      "    clf3 = RandomizedSearchCV(gbc, param_distributions=forest_param_dist, n_iter=100)\n",
      "    #    clf3 = GridSearchCV(gbc, [{'learning_rate': [.01, .03, .1, .3], 'n_estimators': [50, 100, 150],\n",
      "    #                             \"max_depth\": [3, 4, 5]}], cv=5, n_jobs=-1, scoring='roc_auc', verbose=True)\n",
      "    clf3.fit(X_train_3, y_train_3)\n",
      "    clf_3_x_val_predictions = clf3.predict(X_test)\n",
      "    class_rep_3 = classification_report(y_test, clf_3_x_val_predictions)\n",
      "    print clf3.best_params_\n",
      "    print class_rep_3\n",
      "\n",
      "    #joblib.dump(clf, 'model.bin', 5)\n",
      "\n",
      "    # Average predictions from the three classifiers\n",
      "    clf_1_x_test_predictions = clf1.best_estimator_.predict_proba(test_feature_matrix)[:, 1]\n",
      "    clf_2_x_test_predictions = clf2.best_estimator_.predict_proba(test_feature_matrix)[:, 1]\n",
      "    clf_3_x_test_predictions = clf3.best_estimator_.predict_proba(test_feature_matrix)[:, 1]\n",
      "\n",
      "    output_predictions = []\n",
      "    for p in zip(clf_1_x_test_predictions, clf_2_x_test_predictions, clf_3_x_test_predictions):\n",
      "        if p[0] + p[1] + p[2] > 1.25:\n",
      "            output_predictions.append(1)\n",
      "        else:\n",
      "            output_predictions.append(0)\n",
      "\n",
      "\n",
      "    output = zip([x[0] for x in test_ids], output_predictions)\n",
      "    output.insert(0, [\"request_id\", \"requester_received_pizza\"])\n",
      "\n",
      "    output_file = csv.writer(open('predictions.csv', 'w'), delimiter=\",\", quotechar='\"')\n",
      "    for row in output:\n",
      "        output_file.writerow(row)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "LookupError",
       "evalue": "\n**********************************************************************\n  Resource u'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'c:/Users/Connie/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'c:\\\\Users\\\\Connie\\\\Anaconda\\\\nltk_data'\n    - 'c:\\\\Users\\\\Connie\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Connie\\\\AppData\\\\Roaming\\\\nltk_data'\n    - u''\n**********************************************************************",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-3-e65787bad7b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# Train all tf features before messing with the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m     \u001b[0mtf_train_request\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_test_request\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_tfidf_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'request_text_edit_aware'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m     \u001b[0mtf_train_title\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_test_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_tfidf_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'request_title'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-3-e65787bad7b1>\u001b[0m in \u001b[0;36mgenerate_tfidf_matrix\u001b[1;34m(train, test, field_name, _min_df, _max_df)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_min_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_max_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m     \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_text\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtest_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mc:\\Users\\Connie\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m         \"\"\"\n\u001b[1;32m-> 1262\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1263\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mc:\\Users\\Connie\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 817\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mc:\\Users\\Connie\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    746\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mc:\\Users\\Connie\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 234\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-3-e65787bad7b1>\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m     \u001b[0mstems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstem_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstems\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mc:\\Users\\Connie\\Anaconda\\lib\\site-packages\\nltk\\tokenize\\__init__.pyc\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \"\"\"\n\u001b[1;32m--> 101\u001b[1;33m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[0;32m    102\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mc:\\Users\\Connie\\Anaconda\\lib\\site-packages\\nltk\\tokenize\\__init__.pyc\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mc:\\Users\\Connie\\Anaconda\\lib\\site-packages\\nltk\\data.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 781\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mc:\\Users\\Connie\\Anaconda\\lib\\site-packages\\nltk\\data.pyc\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mc:\\Users\\Connie\\Anaconda\\lib\\site-packages\\nltk\\data.pyc\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'c:/Users/Connie/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'c:\\\\Users\\\\Connie\\\\Anaconda\\\\nltk_data'\n    - 'c:\\\\Users\\\\Connie\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Connie\\\\AppData\\\\Roaming\\\\nltk_data'\n    - u''\n**********************************************************************"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}